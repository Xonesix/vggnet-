{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wFVV7X-UXVcB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download THe Data"
      ],
      "metadata": {
        "id": "HFG3K_h6bbGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(root='data',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='data',\n",
        "                           train=False,\n",
        "                           download=True,\n",
        "                           transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pIjYINFb_wR",
        "outputId": "82216a68-b4c3-426c-9e70-e612d923c5b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 14595681.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 448692.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3571804.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9789583.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.data.shape, train_data.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwusgoFvcIzl",
        "outputId": "a431cea4-27df-47d0-d3ac-109fc65faa84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10000, 28, 28]), torch.Size([60000, 28, 28]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loaders =  {\n",
        "    'train': DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "    'test': DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
        "}"
      ],
      "metadata": {
        "id": "LyYqigJccSxD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model architecture\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.softmax(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "FORXP_TCcl6J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN().to(device)"
      ],
      "metadata": {
        "id": "B3xvrWV_d-CO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "2EyEjDB-eAA6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = loss_fn(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 20 == 0:\n",
        "      print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "7Ec-uLECeAZL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  model.eval()\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in loaders['test']:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += loss_fn(output, target).item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct+=pred.eq(target.view_as(pred)).sum().item()\n",
        "  test_loss /= len(loaders['test'].dataset)\n",
        "  print(f'\\nTest set: Average loss: {test_loss}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset)}%)\\n')"
      ],
      "metadata": {
        "id": "wHRTFqzKeuA3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 11):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AqHTS-tfaWX",
        "outputId": "1b81d27a-1d1f-4a69-a06b-412f7d988ffc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-8fb1a2765ca0>:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 Loss: 2.302436590194702\n",
            "Train Epoch: 1 [2000/60000 Loss: 2.2977547645568848\n",
            "Train Epoch: 1 [4000/60000 Loss: 2.202754259109497\n",
            "Train Epoch: 1 [6000/60000 Loss: 2.0243494510650635\n",
            "Train Epoch: 1 [8000/60000 Loss: 1.8913447856903076\n",
            "Train Epoch: 1 [10000/60000 Loss: 1.9135507345199585\n",
            "Train Epoch: 1 [12000/60000 Loss: 1.7987151145935059\n",
            "Train Epoch: 1 [14000/60000 Loss: 1.8211174011230469\n",
            "Train Epoch: 1 [16000/60000 Loss: 1.8177175521850586\n",
            "Train Epoch: 1 [18000/60000 Loss: 1.7361468076705933\n",
            "Train Epoch: 1 [20000/60000 Loss: 1.7374186515808105\n",
            "Train Epoch: 1 [22000/60000 Loss: 1.696853756904602\n",
            "Train Epoch: 1 [24000/60000 Loss: 1.6461868286132812\n",
            "Train Epoch: 1 [26000/60000 Loss: 1.6385493278503418\n",
            "Train Epoch: 1 [28000/60000 Loss: 1.694347858428955\n",
            "Train Epoch: 1 [30000/60000 Loss: 1.6028286218643188\n",
            "Train Epoch: 1 [32000/60000 Loss: 1.6661895513534546\n",
            "Train Epoch: 1 [34000/60000 Loss: 1.6482908725738525\n",
            "Train Epoch: 1 [36000/60000 Loss: 1.6619980335235596\n",
            "Train Epoch: 1 [38000/60000 Loss: 1.6398817300796509\n",
            "Train Epoch: 1 [40000/60000 Loss: 1.6145925521850586\n",
            "Train Epoch: 1 [42000/60000 Loss: 1.640865445137024\n",
            "Train Epoch: 1 [44000/60000 Loss: 1.6034035682678223\n",
            "Train Epoch: 1 [46000/60000 Loss: 1.5959389209747314\n",
            "Train Epoch: 1 [48000/60000 Loss: 1.6203900575637817\n",
            "Train Epoch: 1 [50000/60000 Loss: 1.5967309474945068\n",
            "Train Epoch: 1 [52000/60000 Loss: 1.6057751178741455\n",
            "Train Epoch: 1 [54000/60000 Loss: 1.5670478343963623\n",
            "Train Epoch: 1 [56000/60000 Loss: 1.6094061136245728\n",
            "Train Epoch: 1 [58000/60000 Loss: 1.642594337463379\n",
            "\n",
            "Test set: Average loss: 0.015268694388866424, Accuracy: 9372/10000 (93.72%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 Loss: 1.6148065328598022\n",
            "Train Epoch: 2 [2000/60000 Loss: 1.6358819007873535\n",
            "Train Epoch: 2 [4000/60000 Loss: 1.587100863456726\n",
            "Train Epoch: 2 [6000/60000 Loss: 1.586033582687378\n",
            "Train Epoch: 2 [8000/60000 Loss: 1.5769468545913696\n",
            "Train Epoch: 2 [10000/60000 Loss: 1.593747615814209\n",
            "Train Epoch: 2 [12000/60000 Loss: 1.5838290452957153\n",
            "Train Epoch: 2 [14000/60000 Loss: 1.5941473245620728\n",
            "Train Epoch: 2 [16000/60000 Loss: 1.5623610019683838\n",
            "Train Epoch: 2 [18000/60000 Loss: 1.5627226829528809\n",
            "Train Epoch: 2 [20000/60000 Loss: 1.5925993919372559\n",
            "Train Epoch: 2 [22000/60000 Loss: 1.5854133367538452\n",
            "Train Epoch: 2 [24000/60000 Loss: 1.5885539054870605\n",
            "Train Epoch: 2 [26000/60000 Loss: 1.5461397171020508\n",
            "Train Epoch: 2 [28000/60000 Loss: 1.6062082052230835\n",
            "Train Epoch: 2 [30000/60000 Loss: 1.6086180210113525\n",
            "Train Epoch: 2 [32000/60000 Loss: 1.5643589496612549\n",
            "Train Epoch: 2 [34000/60000 Loss: 1.5631287097930908\n",
            "Train Epoch: 2 [36000/60000 Loss: 1.607029914855957\n",
            "Train Epoch: 2 [38000/60000 Loss: 1.580368161201477\n",
            "Train Epoch: 2 [40000/60000 Loss: 1.6499660015106201\n",
            "Train Epoch: 2 [42000/60000 Loss: 1.5754785537719727\n",
            "Train Epoch: 2 [44000/60000 Loss: 1.5803128480911255\n",
            "Train Epoch: 2 [46000/60000 Loss: 1.5764689445495605\n",
            "Train Epoch: 2 [48000/60000 Loss: 1.612298846244812\n",
            "Train Epoch: 2 [50000/60000 Loss: 1.6238654851913452\n",
            "Train Epoch: 2 [52000/60000 Loss: 1.5796035528182983\n",
            "Train Epoch: 2 [54000/60000 Loss: 1.5667823553085327\n",
            "Train Epoch: 2 [56000/60000 Loss: 1.5637462139129639\n",
            "Train Epoch: 2 [58000/60000 Loss: 1.5816314220428467\n",
            "\n",
            "Test set: Average loss: 0.015098676180839538, Accuracy: 9520/10000 (95.2%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 Loss: 1.5503909587860107\n",
            "Train Epoch: 3 [2000/60000 Loss: 1.549174189567566\n",
            "Train Epoch: 3 [4000/60000 Loss: 1.5626263618469238\n",
            "Train Epoch: 3 [6000/60000 Loss: 1.5510598421096802\n",
            "Train Epoch: 3 [8000/60000 Loss: 1.5275187492370605\n",
            "Train Epoch: 3 [10000/60000 Loss: 1.6014091968536377\n",
            "Train Epoch: 3 [12000/60000 Loss: 1.5878328084945679\n",
            "Train Epoch: 3 [14000/60000 Loss: 1.5448668003082275\n",
            "Train Epoch: 3 [16000/60000 Loss: 1.543528437614441\n",
            "Train Epoch: 3 [18000/60000 Loss: 1.5765031576156616\n",
            "Train Epoch: 3 [20000/60000 Loss: 1.5867507457733154\n",
            "Train Epoch: 3 [22000/60000 Loss: 1.5741629600524902\n",
            "Train Epoch: 3 [24000/60000 Loss: 1.5381343364715576\n",
            "Train Epoch: 3 [26000/60000 Loss: 1.6016954183578491\n",
            "Train Epoch: 3 [28000/60000 Loss: 1.5763477087020874\n",
            "Train Epoch: 3 [30000/60000 Loss: 1.5419061183929443\n",
            "Train Epoch: 3 [32000/60000 Loss: 1.5293866395950317\n",
            "Train Epoch: 3 [34000/60000 Loss: 1.5289913415908813\n",
            "Train Epoch: 3 [36000/60000 Loss: 1.5609797239303589\n",
            "Train Epoch: 3 [38000/60000 Loss: 1.5115346908569336\n",
            "Train Epoch: 3 [40000/60000 Loss: 1.5422897338867188\n",
            "Train Epoch: 3 [42000/60000 Loss: 1.5175198316574097\n",
            "Train Epoch: 3 [44000/60000 Loss: 1.5603604316711426\n",
            "Train Epoch: 3 [46000/60000 Loss: 1.566451907157898\n",
            "Train Epoch: 3 [48000/60000 Loss: 1.5664972066879272\n",
            "Train Epoch: 3 [50000/60000 Loss: 1.5591217279434204\n",
            "Train Epoch: 3 [52000/60000 Loss: 1.606789231300354\n",
            "Train Epoch: 3 [54000/60000 Loss: 1.5601762533187866\n",
            "Train Epoch: 3 [56000/60000 Loss: 1.5600273609161377\n",
            "Train Epoch: 3 [58000/60000 Loss: 1.5171490907669067\n",
            "\n",
            "Test set: Average loss: 0.015005879378318786, Accuracy: 9611/10000 (96.11%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 Loss: 1.51676344871521\n",
            "Train Epoch: 4 [2000/60000 Loss: 1.550339698791504\n",
            "Train Epoch: 4 [4000/60000 Loss: 1.5243728160858154\n",
            "Train Epoch: 4 [6000/60000 Loss: 1.521195411682129\n",
            "Train Epoch: 4 [8000/60000 Loss: 1.5534039735794067\n",
            "Train Epoch: 4 [10000/60000 Loss: 1.5827692747116089\n",
            "Train Epoch: 4 [12000/60000 Loss: 1.5254417657852173\n",
            "Train Epoch: 4 [14000/60000 Loss: 1.5382710695266724\n",
            "Train Epoch: 4 [16000/60000 Loss: 1.5570160150527954\n",
            "Train Epoch: 4 [18000/60000 Loss: 1.5272588729858398\n",
            "Train Epoch: 4 [20000/60000 Loss: 1.5721254348754883\n",
            "Train Epoch: 4 [22000/60000 Loss: 1.6165951490402222\n",
            "Train Epoch: 4 [24000/60000 Loss: 1.5199967622756958\n",
            "Train Epoch: 4 [26000/60000 Loss: 1.5560511350631714\n",
            "Train Epoch: 4 [28000/60000 Loss: 1.5151602029800415\n",
            "Train Epoch: 4 [30000/60000 Loss: 1.5455325841903687\n",
            "Train Epoch: 4 [32000/60000 Loss: 1.5548161268234253\n",
            "Train Epoch: 4 [34000/60000 Loss: 1.5678523778915405\n",
            "Train Epoch: 4 [36000/60000 Loss: 1.572805643081665\n",
            "Train Epoch: 4 [38000/60000 Loss: 1.5164316892623901\n",
            "Train Epoch: 4 [40000/60000 Loss: 1.5621247291564941\n",
            "Train Epoch: 4 [42000/60000 Loss: 1.5300514698028564\n",
            "Train Epoch: 4 [44000/60000 Loss: 1.5314762592315674\n",
            "Train Epoch: 4 [46000/60000 Loss: 1.5752748250961304\n",
            "Train Epoch: 4 [48000/60000 Loss: 1.5280681848526\n",
            "Train Epoch: 4 [50000/60000 Loss: 1.5774589776992798\n",
            "Train Epoch: 4 [52000/60000 Loss: 1.5679806470870972\n",
            "Train Epoch: 4 [54000/60000 Loss: 1.5447009801864624\n",
            "Train Epoch: 4 [56000/60000 Loss: 1.5470231771469116\n",
            "Train Epoch: 4 [58000/60000 Loss: 1.497103214263916\n",
            "\n",
            "Test set: Average loss: 0.014974065589904785, Accuracy: 9643/10000 (96.43%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 Loss: 1.510921835899353\n",
            "Train Epoch: 5 [2000/60000 Loss: 1.5475503206253052\n",
            "Train Epoch: 5 [4000/60000 Loss: 1.5200389623641968\n",
            "Train Epoch: 5 [6000/60000 Loss: 1.529991626739502\n",
            "Train Epoch: 5 [8000/60000 Loss: 1.5353399515151978\n",
            "Train Epoch: 5 [10000/60000 Loss: 1.5225986242294312\n",
            "Train Epoch: 5 [12000/60000 Loss: 1.5829750299453735\n",
            "Train Epoch: 5 [14000/60000 Loss: 1.5980721712112427\n",
            "Train Epoch: 5 [16000/60000 Loss: 1.540283203125\n",
            "Train Epoch: 5 [18000/60000 Loss: 1.5569502115249634\n",
            "Train Epoch: 5 [20000/60000 Loss: 1.5213090181350708\n",
            "Train Epoch: 5 [22000/60000 Loss: 1.5414401292800903\n",
            "Train Epoch: 5 [24000/60000 Loss: 1.5625957250595093\n",
            "Train Epoch: 5 [26000/60000 Loss: 1.536901831626892\n",
            "Train Epoch: 5 [28000/60000 Loss: 1.53977632522583\n",
            "Train Epoch: 5 [30000/60000 Loss: 1.5496611595153809\n",
            "Train Epoch: 5 [32000/60000 Loss: 1.5192135572433472\n",
            "Train Epoch: 5 [34000/60000 Loss: 1.5442646741867065\n",
            "Train Epoch: 5 [36000/60000 Loss: 1.5086398124694824\n",
            "Train Epoch: 5 [38000/60000 Loss: 1.5097514390945435\n",
            "Train Epoch: 5 [40000/60000 Loss: 1.5320725440979004\n",
            "Train Epoch: 5 [42000/60000 Loss: 1.5589736700057983\n",
            "Train Epoch: 5 [44000/60000 Loss: 1.5421501398086548\n",
            "Train Epoch: 5 [46000/60000 Loss: 1.5300744771957397\n",
            "Train Epoch: 5 [48000/60000 Loss: 1.500913143157959\n",
            "Train Epoch: 5 [50000/60000 Loss: 1.564042568206787\n",
            "Train Epoch: 5 [52000/60000 Loss: 1.5394665002822876\n",
            "Train Epoch: 5 [54000/60000 Loss: 1.5155916213989258\n",
            "Train Epoch: 5 [56000/60000 Loss: 1.5360645055770874\n",
            "Train Epoch: 5 [58000/60000 Loss: 1.514801025390625\n",
            "\n",
            "Test set: Average loss: 0.014929903948307037, Accuracy: 9685/10000 (96.85%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 Loss: 1.569891095161438\n",
            "Train Epoch: 6 [2000/60000 Loss: 1.5215541124343872\n",
            "Train Epoch: 6 [4000/60000 Loss: 1.5647767782211304\n",
            "Train Epoch: 6 [6000/60000 Loss: 1.5215322971343994\n",
            "Train Epoch: 6 [8000/60000 Loss: 1.5374871492385864\n",
            "Train Epoch: 6 [10000/60000 Loss: 1.565466284751892\n",
            "Train Epoch: 6 [12000/60000 Loss: 1.5167285203933716\n",
            "Train Epoch: 6 [14000/60000 Loss: 1.5678921937942505\n",
            "Train Epoch: 6 [16000/60000 Loss: 1.5453022718429565\n",
            "Train Epoch: 6 [18000/60000 Loss: 1.532283902168274\n",
            "Train Epoch: 6 [20000/60000 Loss: 1.5670678615570068\n",
            "Train Epoch: 6 [22000/60000 Loss: 1.5028191804885864\n",
            "Train Epoch: 6 [24000/60000 Loss: 1.5236308574676514\n",
            "Train Epoch: 6 [26000/60000 Loss: 1.5491336584091187\n",
            "Train Epoch: 6 [28000/60000 Loss: 1.5455163717269897\n",
            "Train Epoch: 6 [30000/60000 Loss: 1.5578925609588623\n",
            "Train Epoch: 6 [32000/60000 Loss: 1.517012357711792\n",
            "Train Epoch: 6 [34000/60000 Loss: 1.5311449766159058\n",
            "Train Epoch: 6 [36000/60000 Loss: 1.5450161695480347\n",
            "Train Epoch: 6 [38000/60000 Loss: 1.5173935890197754\n",
            "Train Epoch: 6 [40000/60000 Loss: 1.5394171476364136\n",
            "Train Epoch: 6 [42000/60000 Loss: 1.5099151134490967\n",
            "Train Epoch: 6 [44000/60000 Loss: 1.6049232482910156\n",
            "Train Epoch: 6 [46000/60000 Loss: 1.52821946144104\n",
            "Train Epoch: 6 [48000/60000 Loss: 1.511430025100708\n",
            "Train Epoch: 6 [50000/60000 Loss: 1.524635672569275\n",
            "Train Epoch: 6 [52000/60000 Loss: 1.5201507806777954\n",
            "Train Epoch: 6 [54000/60000 Loss: 1.520580768585205\n",
            "Train Epoch: 6 [56000/60000 Loss: 1.5790016651153564\n",
            "Train Epoch: 6 [58000/60000 Loss: 1.5304656028747559\n",
            "\n",
            "Test set: Average loss: 0.014895485532283783, Accuracy: 9716/10000 (97.16%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 Loss: 1.5166538953781128\n",
            "Train Epoch: 7 [2000/60000 Loss: 1.556272268295288\n",
            "Train Epoch: 7 [4000/60000 Loss: 1.530708909034729\n",
            "Train Epoch: 7 [6000/60000 Loss: 1.5375542640686035\n",
            "Train Epoch: 7 [8000/60000 Loss: 1.5396275520324707\n",
            "Train Epoch: 7 [10000/60000 Loss: 1.518700122833252\n",
            "Train Epoch: 7 [12000/60000 Loss: 1.577757716178894\n",
            "Train Epoch: 7 [14000/60000 Loss: 1.5618369579315186\n",
            "Train Epoch: 7 [16000/60000 Loss: 1.4814753532409668\n",
            "Train Epoch: 7 [18000/60000 Loss: 1.513848900794983\n",
            "Train Epoch: 7 [20000/60000 Loss: 1.543369174003601\n",
            "Train Epoch: 7 [22000/60000 Loss: 1.5151768922805786\n",
            "Train Epoch: 7 [24000/60000 Loss: 1.4765994548797607\n",
            "Train Epoch: 7 [26000/60000 Loss: 1.5512601137161255\n",
            "Train Epoch: 7 [28000/60000 Loss: 1.5376113653182983\n",
            "Train Epoch: 7 [30000/60000 Loss: 1.5626243352890015\n",
            "Train Epoch: 7 [32000/60000 Loss: 1.5396875143051147\n",
            "Train Epoch: 7 [34000/60000 Loss: 1.5167105197906494\n",
            "Train Epoch: 7 [36000/60000 Loss: 1.4901313781738281\n",
            "Train Epoch: 7 [38000/60000 Loss: 1.545883297920227\n",
            "Train Epoch: 7 [40000/60000 Loss: 1.5660525560379028\n",
            "Train Epoch: 7 [42000/60000 Loss: 1.5022555589675903\n",
            "Train Epoch: 7 [44000/60000 Loss: 1.5391660928726196\n",
            "Train Epoch: 7 [46000/60000 Loss: 1.5103684663772583\n",
            "Train Epoch: 7 [48000/60000 Loss: 1.54597806930542\n",
            "Train Epoch: 7 [50000/60000 Loss: 1.5101836919784546\n",
            "Train Epoch: 7 [52000/60000 Loss: 1.5790873765945435\n",
            "Train Epoch: 7 [54000/60000 Loss: 1.5574424266815186\n",
            "Train Epoch: 7 [56000/60000 Loss: 1.5300378799438477\n",
            "Train Epoch: 7 [58000/60000 Loss: 1.5162020921707153\n",
            "\n",
            "Test set: Average loss: 0.01489478634595871, Accuracy: 9721/10000 (97.21%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 Loss: 1.520012378692627\n",
            "Train Epoch: 8 [2000/60000 Loss: 1.5661808252334595\n",
            "Train Epoch: 8 [4000/60000 Loss: 1.5431469678878784\n",
            "Train Epoch: 8 [6000/60000 Loss: 1.519520878791809\n",
            "Train Epoch: 8 [8000/60000 Loss: 1.5590183734893799\n",
            "Train Epoch: 8 [10000/60000 Loss: 1.5176644325256348\n",
            "Train Epoch: 8 [12000/60000 Loss: 1.523655652999878\n",
            "Train Epoch: 8 [14000/60000 Loss: 1.5551801919937134\n",
            "Train Epoch: 8 [16000/60000 Loss: 1.5446544885635376\n",
            "Train Epoch: 8 [18000/60000 Loss: 1.508390188217163\n",
            "Train Epoch: 8 [20000/60000 Loss: 1.5209438800811768\n",
            "Train Epoch: 8 [22000/60000 Loss: 1.5596345663070679\n",
            "Train Epoch: 8 [24000/60000 Loss: 1.494013786315918\n",
            "Train Epoch: 8 [26000/60000 Loss: 1.5147594213485718\n",
            "Train Epoch: 8 [28000/60000 Loss: 1.528028964996338\n",
            "Train Epoch: 8 [30000/60000 Loss: 1.5413658618927002\n",
            "Train Epoch: 8 [32000/60000 Loss: 1.5260369777679443\n",
            "Train Epoch: 8 [34000/60000 Loss: 1.5059983730316162\n",
            "Train Epoch: 8 [36000/60000 Loss: 1.5767289400100708\n",
            "Train Epoch: 8 [38000/60000 Loss: 1.529647707939148\n",
            "Train Epoch: 8 [40000/60000 Loss: 1.5426043272018433\n",
            "Train Epoch: 8 [42000/60000 Loss: 1.5098921060562134\n",
            "Train Epoch: 8 [44000/60000 Loss: 1.5153801441192627\n",
            "Train Epoch: 8 [46000/60000 Loss: 1.5445812940597534\n",
            "Train Epoch: 8 [48000/60000 Loss: 1.5157980918884277\n",
            "Train Epoch: 8 [50000/60000 Loss: 1.5095672607421875\n",
            "Train Epoch: 8 [52000/60000 Loss: 1.5197579860687256\n",
            "Train Epoch: 8 [54000/60000 Loss: 1.5275486707687378\n",
            "Train Epoch: 8 [56000/60000 Loss: 1.5644663572311401\n",
            "Train Epoch: 8 [58000/60000 Loss: 1.5209481716156006\n",
            "\n",
            "Test set: Average loss: 0.01486855537891388, Accuracy: 9746/10000 (97.46%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 Loss: 1.5455355644226074\n",
            "Train Epoch: 9 [2000/60000 Loss: 1.5456771850585938\n",
            "Train Epoch: 9 [4000/60000 Loss: 1.5222423076629639\n",
            "Train Epoch: 9 [6000/60000 Loss: 1.4976370334625244\n",
            "Train Epoch: 9 [8000/60000 Loss: 1.510389804840088\n",
            "Train Epoch: 9 [10000/60000 Loss: 1.4928252696990967\n",
            "Train Epoch: 9 [12000/60000 Loss: 1.5253945589065552\n",
            "Train Epoch: 9 [14000/60000 Loss: 1.5222363471984863\n",
            "Train Epoch: 9 [16000/60000 Loss: 1.5287452936172485\n",
            "Train Epoch: 9 [18000/60000 Loss: 1.49412202835083\n",
            "Train Epoch: 9 [20000/60000 Loss: 1.528889775276184\n",
            "Train Epoch: 9 [22000/60000 Loss: 1.56154465675354\n",
            "Train Epoch: 9 [24000/60000 Loss: 1.5165530443191528\n",
            "Train Epoch: 9 [26000/60000 Loss: 1.564178466796875\n",
            "Train Epoch: 9 [28000/60000 Loss: 1.5179011821746826\n",
            "Train Epoch: 9 [30000/60000 Loss: 1.5569554567337036\n",
            "Train Epoch: 9 [32000/60000 Loss: 1.4976153373718262\n",
            "Train Epoch: 9 [34000/60000 Loss: 1.5132485628128052\n",
            "Train Epoch: 9 [36000/60000 Loss: 1.5549734830856323\n",
            "Train Epoch: 9 [38000/60000 Loss: 1.5203646421432495\n",
            "Train Epoch: 9 [40000/60000 Loss: 1.5319005250930786\n",
            "Train Epoch: 9 [42000/60000 Loss: 1.532161831855774\n",
            "Train Epoch: 9 [44000/60000 Loss: 1.5126365423202515\n",
            "Train Epoch: 9 [46000/60000 Loss: 1.5141032934188843\n",
            "Train Epoch: 9 [48000/60000 Loss: 1.5702793598175049\n",
            "Train Epoch: 9 [50000/60000 Loss: 1.4985162019729614\n",
            "Train Epoch: 9 [52000/60000 Loss: 1.4803509712219238\n",
            "Train Epoch: 9 [54000/60000 Loss: 1.5642300844192505\n",
            "Train Epoch: 9 [56000/60000 Loss: 1.560089111328125\n",
            "Train Epoch: 9 [58000/60000 Loss: 1.5442472696304321\n",
            "\n",
            "Test set: Average loss: 0.014867106878757476, Accuracy: 9745/10000 (97.45%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 Loss: 1.5372546911239624\n",
            "Train Epoch: 10 [2000/60000 Loss: 1.5160363912582397\n",
            "Train Epoch: 10 [4000/60000 Loss: 1.5617443323135376\n",
            "Train Epoch: 10 [6000/60000 Loss: 1.5079940557479858\n",
            "Train Epoch: 10 [8000/60000 Loss: 1.488680124282837\n",
            "Train Epoch: 10 [10000/60000 Loss: 1.5490397214889526\n",
            "Train Epoch: 10 [12000/60000 Loss: 1.5211377143859863\n",
            "Train Epoch: 10 [14000/60000 Loss: 1.5297517776489258\n",
            "Train Epoch: 10 [16000/60000 Loss: 1.531348705291748\n",
            "Train Epoch: 10 [18000/60000 Loss: 1.556635856628418\n",
            "Train Epoch: 10 [20000/60000 Loss: 1.545711636543274\n",
            "Train Epoch: 10 [22000/60000 Loss: 1.4824520349502563\n",
            "Train Epoch: 10 [24000/60000 Loss: 1.507625699043274\n",
            "Train Epoch: 10 [26000/60000 Loss: 1.5102779865264893\n",
            "Train Epoch: 10 [28000/60000 Loss: 1.5306010246276855\n",
            "Train Epoch: 10 [30000/60000 Loss: 1.5208145380020142\n",
            "Train Epoch: 10 [32000/60000 Loss: 1.5466675758361816\n",
            "Train Epoch: 10 [34000/60000 Loss: 1.5050891637802124\n",
            "Train Epoch: 10 [36000/60000 Loss: 1.4992756843566895\n",
            "Train Epoch: 10 [38000/60000 Loss: 1.5078125\n",
            "Train Epoch: 10 [40000/60000 Loss: 1.5570799112319946\n",
            "Train Epoch: 10 [42000/60000 Loss: 1.5036778450012207\n",
            "Train Epoch: 10 [44000/60000 Loss: 1.53361177444458\n",
            "Train Epoch: 10 [46000/60000 Loss: 1.5562233924865723\n",
            "Train Epoch: 10 [48000/60000 Loss: 1.5039478540420532\n",
            "Train Epoch: 10 [50000/60000 Loss: 1.5457732677459717\n",
            "Train Epoch: 10 [52000/60000 Loss: 1.50807523727417\n",
            "Train Epoch: 10 [54000/60000 Loss: 1.523282766342163\n",
            "Train Epoch: 10 [56000/60000 Loss: 1.5316932201385498\n",
            "Train Epoch: 10 [58000/60000 Loss: 1.5154168605804443\n",
            "\n",
            "Test set: Average loss: 0.01485539345741272, Accuracy: 9757/10000 (97.57%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TTMQaazJfrET"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "data,target = test_data[3]\n",
        "data = data.unsqueeze(0).to(device)\n",
        "output = model(data)\n",
        "\n",
        "prediction = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "print(f'Prediction: {prediction}')\n",
        "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "iey9dg8EgaxG",
        "outputId": "01b1e842-daf3-4971-f983-8042b8669c29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-8fb1a2765ca0>:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[0]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wir9ltPkgq5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}